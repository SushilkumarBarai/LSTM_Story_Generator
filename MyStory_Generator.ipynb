{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0210f327-cb91-44e5-8bce-67ac93d3724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "class StoryGenerator:\n",
    "    def __init__(self, sequence_length=40, max_vocab_size=10000):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.initialize_directories()  # Create necessary directories\n",
    "\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "\n",
    "    def initialize_directories(self):\n",
    "        os.makedirs('models', exist_ok=True)         \n",
    "        os.makedirs('tokenizers', exist_ok=True)     \n",
    "\n",
    "    def load_text_in_chunks(self, file_path, chunk_size=1024 * 1024):  # 1MB chunks\n",
    "        \"\"\"Generator function to read a file in chunks.\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            while True:\n",
    "                chunk = f.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                yield chunk\n",
    "\n",
    "    def preprocess_text(self, text, user_id):\n",
    "        tokenizer = Tokenizer(num_words=self.max_vocab_size, oov_token='<OOV>')\n",
    "        tokenizer.fit_on_texts([text])\n",
    "        total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "        input_sequences = []\n",
    "        for line in text.split('. '):\n",
    "            token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "            for i in range(1, len(token_list)):\n",
    "                n_gram_sequence = token_list[:i+1]\n",
    "                input_sequences.append(n_gram_sequence)\n",
    "\n",
    "        input_sequences = np.array(pad_sequences(input_sequences, maxlen=self.sequence_length, padding='pre'))\n",
    "\n",
    "        X = input_sequences[:, :-1]\n",
    "        y = input_sequences[:, -1]\n",
    "        y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "\n",
    "        self.save_tokenizer(tokenizer, user_id)\n",
    "\n",
    "        return X, y, total_words\n",
    "\n",
    "    def build_model(self, total_words):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(total_words, 100, input_length=self.sequence_length-1))\n",
    "        model.add(LSTM(150, return_sequences=True))\n",
    "        model.add(LSTM(100))\n",
    "        model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        print(\"Model built successfully!\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train_model(self, user_id, text_file_path, epochs=30, batch_size=32):\n",
    "        \"\"\"Train the model on large files by processing in chunks.\"\"\"\n",
    "        print(\"Loading and preprocessing text data for a user:\", user_id)\n",
    "        total_words = 0\n",
    "        all_X, all_y = [], []\n",
    "        \n",
    "        # Process file in chunks\n",
    "        for chunk in self.load_text_in_chunks(text_file_path):\n",
    "            X, y, total_words_chunk = self.preprocess_text(chunk, user_id)\n",
    "            all_X.append(X)\n",
    "            all_y.append(y)\n",
    "            total_words = max(total_words, total_words_chunk)\n",
    "\n",
    "        # Concatenate all data\n",
    "        all_X = np.vstack(all_X)\n",
    "        all_y = np.vstack(all_y)\n",
    "\n",
    "        # Build the model\n",
    "        print(f\"Building the model for user {user_id}...\")\n",
    "        model = self.build_model(total_words)\n",
    "\n",
    "        # Train the model\n",
    "        print(f\"Training the model for user {user_id}...\")\n",
    "        try:\n",
    "            model.fit(all_X, all_y, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during training: {e}\")\n",
    "            return\n",
    "        \n",
    "        # Save the model after training\n",
    "        self.save_model(model, user_id)\n",
    "\n",
    "    def save_model(self, model, user_id):\n",
    "        model_path = f'models/story_generator_{user_id}.h5'\n",
    "        model.save(model_path)\n",
    "        print(f\"Model for user {user_id} saved to {model_path}\")\n",
    "\n",
    "    def load_model(self, user_id):\n",
    "        model_path = f'models/story_generator_{user_id}.h5'\n",
    "        if os.path.exists(model_path):\n",
    "            model = load_model(model_path)\n",
    "            print(f\"Model for user {user_id} loaded from {model_path}\")\n",
    "            return model\n",
    "        else:\n",
    "            print(f\"No model found for user {user_id}. Please train a new model first.\")\n",
    "            return None\n",
    "\n",
    "    def save_tokenizer(self, tokenizer, user_id):\n",
    "        tokenizer_path = f'tokenizers/tokenizer_{user_id}.pkl'\n",
    "        with open(tokenizer_path, 'wb') as f:\n",
    "            pickle.dump(tokenizer, f)\n",
    "        print(f\"Tokenizer for user {user_id} saved to {tokenizer_path}\")\n",
    "\n",
    "    def load_tokenizer(self, user_id):\n",
    "        tokenizer_path = f'tokenizers/tokenizer_{user_id}.pkl'\n",
    "        if os.path.exists(tokenizer_path):\n",
    "            with open(tokenizer_path, 'rb') as f:\n",
    "                tokenizer = pickle.load(f)\n",
    "            print(f\"Tokenizer for user {user_id} loaded from {tokenizer_path}\")\n",
    "            return tokenizer\n",
    "        else:\n",
    "            print(f\"No tokenizer found for user {user_id}. Please train a new model first.\")\n",
    "            return None\n",
    "\n",
    "    def generate_story(self, user_id, seed_text, max_words=100):\n",
    "        model = self.load_model(user_id)\n",
    "        tokenizer = self.load_tokenizer(user_id)\n",
    "\n",
    "        if model is None or tokenizer is None:\n",
    "            print(f\"Model or tokenizer not found for user {user_id}. Please train the model first.\")\n",
    "            return None\n",
    "\n",
    "        logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "        for _ in range(max_words):\n",
    "            token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "            token_list = pad_sequences([token_list], maxlen=self.sequence_length-1, padding='pre')\n",
    "            predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
    "\n",
    "            output_word = \"\"\n",
    "            for word, index in tokenizer.word_index.items():\n",
    "                if index == predicted:\n",
    "                    output_word = word\n",
    "                    break\n",
    "            if output_word:  # Check if output_word is found\n",
    "                seed_text += \" \" + output_word\n",
    "            else:\n",
    "                break  # Stop if no valid word is predicted\n",
    "\n",
    "        return seed_text\n",
    "\n",
    "\n",
    "# Example usage of the StoryGenerator class for a specific user\n",
    "story_gen = StoryGenerator()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b93a70-4093-4a73-879a-507491ebc50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train the model for a user (user_id='user123')\n",
    "path=\"/<path>/Story_generation/grandma_stories.txt\"\n",
    "story_gen.train_model(user_id='user123', text_file_path=path, epochs=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d5245-e037-484b-b3c8-6ea5148a08f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To generate a story for the user\n",
    "story = story_gen.generate_story(user_id='user123', seed_text=\"Once upon a time\", max_words=100)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c049de8-d36b-45bc-bc7d-e3ed30fb98ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
